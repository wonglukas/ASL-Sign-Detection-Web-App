{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jamelof23/ASL2/blob/main/asl100_cleaned.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UCPlIsNhCmnE"
   },
   "source": [
    "# Install compatible versions of dependent libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HtUYhaNRAY8V"
   },
   "outputs": [],
   "source": [
    "# Uninstall existing packages that may cause conflicts\n",
    "!pip uninstall -y tensorflow tensorflow-metadata\n",
    "\n",
    "# Install specific compatible versions\n",
    "!pip install tensorflow==2.11.0\n",
    "!pip install mediapipe==0.10.15\n",
    "!pip install protobuf==3.20.3\n",
    "\n",
    "import tensorflow as tf\n",
    "import mediapipe as mp\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"MediaPipe version:\", mp.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pUOBb1nVwTyj"
   },
   "source": [
    "# Upload dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5eXt1gToxNf7"
   },
   "source": [
    "Upload archive.zip to /content/sample_data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qFMLQcgOwgQX"
   },
   "source": [
    "# Unzip the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rLe8-fFDehIb"
   },
   "outputs": [],
   "source": [
    "!unzip /content/sample_data/archive.zip -d /content/asl_alphabet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "38-XrctGH4ac"
   },
   "source": [
    "# Detect hand landmarks in training images using mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "grFspfka0R2T"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import os\n",
    "import numpy as np\n",
    "from google.colab.patches import cv2_imshow\n",
    "\n",
    "# Initialize MediaPipe hands\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=True, max_num_hands=1, min_detection_confidence=0.2)\n",
    "\n",
    "# Define directories\n",
    "train_dir = '/content/asl_alphabet/asl_alphabet_train/asl_alphabet_train/'\n",
    "output_dir = '/content/asl_alphabet/hand_landmarks/'\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# To track the first image processed for each class\n",
    "saved_classes = set()\n",
    "output_summary = []\n",
    "\n",
    "# Process images in the training directory\n",
    "for class_name in os.listdir(train_dir):\n",
    "    class_dir = os.path.join(train_dir, class_name)\n",
    "\n",
    "    for image_name in os.listdir(class_dir):\n",
    "        image_path = os.path.join(class_dir, image_name)\n",
    "\n",
    "        # Read the image\n",
    "        image = cv2.imread(image_path)\n",
    "\n",
    "        # Check if the image is read correctly\n",
    "        if image is None:\n",
    "            print(f\"Error reading image: {image_path}\")\n",
    "            continue\n",
    "\n",
    "        # Convert image to RGB for MediaPipe processing\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Process the image to find hands\n",
    "        results = hands.process(image_rgb)\n",
    "\n",
    "        # Check if hands are detected\n",
    "        if results.multi_hand_landmarks:\n",
    "            # Store the landmarks\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                # Draw hand landmarks on the image for visualization\n",
    "                mp.solutions.drawing_utils.draw_landmarks(\n",
    "                    image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "                # Create a list of landmarks\n",
    "                landmarks = [(lm.x, lm.y, lm.z) for lm in hand_landmarks.landmark]\n",
    "                landmarks_array = np.array(landmarks).flatten()  # Flatten the landmarks\n",
    "\n",
    "                # Save landmarks as a numpy array\n",
    "                output_file = os.path.join(output_dir, f\"{class_name}_{image_name}.npy\")\n",
    "                np.save(output_file, landmarks_array)\n",
    "\n",
    "            # Save the first image of each class with detected hands\n",
    "            if class_name not in saved_classes:\n",
    "                print(class_name)  # Add this line to print the class name\n",
    "                cv2_imshow(image)  # Display the image\n",
    "                cv2.waitKey(1)  # Show the image briefly without waiting for key press\n",
    "                saved_classes.add(class_name)  # Mark this class as processed\n",
    "\n",
    "            output_summary.append(f\"{image_name}: Hands detected\")\n",
    "\n",
    "        else:\n",
    "            output_summary.append(f\"{image_name}: No hands detected\")\n",
    "\n",
    "# Write summary to a file\n",
    "with open('output_summary.txt', 'w') as f:\n",
    "    for line in output_summary:\n",
    "        f.write(line + \"\\n\")\n",
    "\n",
    "# Release the MediaPipe hands object\n",
    "hands.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xRz8z1l_aLsq"
   },
   "source": [
    "# Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BNuaCCVsasf5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define directories\n",
    "landmark_dir = '/content/asl_alphabet/hand_landmarks/'\n",
    "\n",
    "# Define class labels\n",
    "labels = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'nothing', 'space', 'delete']\n",
    "\n",
    "# Prepare data and labels\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for label in labels:\n",
    "    class_files = [f for f in os.listdir(landmark_dir) if f.startswith(label)]\n",
    "\n",
    "    for file_name in class_files:\n",
    "        file_path = os.path.join(landmark_dir, file_name)\n",
    "        landmarks = np.load(file_path)\n",
    "        X.append(landmarks)\n",
    "        y.append(label)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Print shape of data\n",
    "print(f\"Shape of X (landmarks): {X.shape}\")\n",
    "print(f\"Shape of y (labels): {y.shape}\")\n",
    "\n",
    "# Print first few samples to validate\n",
    "print(\"\\nSample landmarks from X:\")\n",
    "print(X[:3])  # Print the first 3 samples of X\n",
    "\n",
    "print(\"\\nSample labels from y:\")\n",
    "print(y[:3])  # Print the first 3 labels from y\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Print shapes after splitting\n",
    "print(f\"\\nShape of X_train: {X_train.shape}\")\n",
    "print(f\"Shape of y_train: {y_train.shape}\")\n",
    "print(f\"Shape of X_val: {X_val.shape}\")\n",
    "print(f\"Shape of y_val: {y_val.shape}\")\n",
    "\n",
    "# Print number of samples per class in the training set\n",
    "from collections import Counter\n",
    "print(\"\\nNumber of samples per class in training set:\")\n",
    "print(Counter(y_train))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "683q0uHTd8xr"
   },
   "source": [
    "# Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HQWBscpTcZep"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform the training and validation labels\n",
    "y_train_enc = label_encoder.fit_transform(y_train)\n",
    "y_val_enc = label_encoder.transform(y_val)\n",
    "\n",
    "# Encode the labels\n",
    "y_train_onehot = to_categorical(y_train_enc, num_classes=len(labels))\n",
    "y_val_onehot = to_categorical(y_val_enc, num_classes=len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "22WvuJPYgxsh"
   },
   "source": [
    "# Define the Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o_AkfIH4Y5Gu"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "#Define the model\n",
    "model = Sequential([\n",
    "    Dense(256, input_shape=(X_train.shape[1],), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4),\n",
    "    Dense(64, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Dense(len(labels), activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7u-1nH0Lg4Au"
   },
   "source": [
    "# Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TS-MbZ5wZEGF"
   },
   "outputs": [],
   "source": [
    "#Set up early stopping\n",
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MLTs0tUBfxYX"
   },
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MWND4UZNZRre"
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train, y_train_onehot,\n",
    "                    epochs=50,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(X_val, y_val_onehot),\n",
    "                    callbacks=[early_stopping])\n",
    "\n",
    "# Print summary of the model training\n",
    "print(\"Model training complete.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHXxPBAMh-iF"
   },
   "source": [
    "# Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "STCMFX7siLBv"
   },
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "model_save_path = '/content/asl_hand_landmarks_model.h5'\n",
    "model.save(model_save_path)\n",
    "\n",
    "print(f\"Model saved to {model_save_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMn7gZW9jTPLC6KkcudZvOG",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
